######################################
# Created by: Allison Bertie Johnson 
# Working Directory: /home/abj15/Research.Project/Run/GenHap/6.3.2019
# Mannual:https://github.com/andrea-tango/GenHap
# GenHap: 6.3.2019
#######################################
# Reinstalled GenHap in Software directory 

# Example script:
# mpiexec -np 4 ./GenHap -i Models/Freq_100/10000SNPs_60x/PacBio/Model_0.wif -o Output/Freq_100/10000SNPs_60x/PacBio
#


[abj15@login2 GenHap-master]$ ls
compile.sh  LICENSE  Models  Models.zip  README.md  src
[abj15@login2 GenHap-master]$ cd /home/abj15/Research.Project/Run/GenHap/6
-bash: cd: /home/abj15/Research.Project/Run/GenHap/6: No such file or directory
[abj15@login2 GenHap-master]$ ls
compile.sh  LICENSE  Models  Models.zip  README.md  src
[abj15@login2 GenHap-master]$ cd /home/abj15/Research.Project/Run/GenHap/
[abj15@login2 GenHap]$ ls
5.26.2019
[abj15@login2 GenHap]$ mkdir 6.3.2019
[abj15@login2 GenHap]$ ls
5.26.2019  6.3.2019
[abj15@login2 GenHap]$ cd 6.3.2019/
[abj15@login2 6.3.2019]$ mpiexec -np 4 /home/abj15/Research.Project/Software/GenHap/GenHap-master/./compile.sh -i /home/abj15/Research.Project/Software/GenHap/GenHap-master/Models/Freq_100//10000SNPs_60x/PacBio//Model_0.wif -o Freq.100_10000SNPs.60x.PacBio
--------------------------------------------------------------------------
mpiexec noticed that the job aborted, but has no info as to the process
that caused that situation.
--------------------------------------------------------------------------
[abj15@login2 6.3.2019]$ ls
[abj15@login2 6.3.2019]$ mpiexec -np 4 /home/abj15/Research.Project/Software/GenHap/GenHap-master/./compile.sh -o Freq.100_10000SNPs.60x.PacBio                        --------------------------------------------------------------------------
mpiexec noticed that the job aborted, but has no info as to the process
that caused that situation.
--------------------------------------------------------------------------
[abj15@login2 6.3.2019]$ mpiexec /home/abj15/Research.Project/Software/GenHap/GenHap-master/./compile.sh -o Freq.100_10000SNPs.60x.PacBio
--------------------------------------------------------------------------
mpiexec noticed that the job aborted, but has no info as to the process
that caused that situation.
--------------------------------------------------------------------------
[abj15@login2 6.3.2019]$ mpiexec
mpiexec (OpenRTE) 1.6.2

Usage: mpiexec [OPTION]...  [PROGRAM]...
Start the given program using Open RTE

   -am <arg0>            Aggregate MCA parameter set file list
   --app <arg0>          Provide an appfile; ignore all other command line
                         options
   -bind-to-board|--bind-to-board
                         Whether to bind processes to specific boards
                         (meaningless on 1 board/node)
   -bind-to-core|--bind-to-core
                         Whether to bind processes to specific cores
   -bind-to-none|--bind-to-none
                         Do not bind processes to cores or sockets
                         (default)
   -bind-to-socket|--bind-to-socket
                         Whether to bind processes to sockets
   -byboard|--byboard    Whether to assign processes round-robin by board
                         (equivalent to bynode if only 1 board/node)
   -bycore|--bycore      Alias for byslot
   -bynode|--bynode      Whether to assign processes round-robin by node
   -byslot|--byslot      Whether to assign processes round-robin by slot
                         (the default)
   -bysocket|--bysocket  Whether to assign processes round-robin by socket
-c|-np|--np <arg0>       Number of processes to run
   -cf|--cartofile <arg0>
                         Provide a cartography file
   -cpus-per-proc|--cpus-per-proc <arg0>
                         Number of cpus to use for each process [default=1]
   -cpus-per-rank|--cpus-per-rank <arg0>
                         Synonym for cpus-per-proc
-d|-debug-devel|--debug-devel
                         Enable debugging of OpenRTE
   -debug|--debug        Invoke the user-level debugger indicated by the
                         orte_base_user_debugger MCA parameter
   -debug-daemons|--debug-daemons
                         Enable debugging of any OpenRTE daemons used by
                         this application
   -debug-daemons-file|--debug-daemons-file
                         Enable debugging of any OpenRTE daemons used by
                         this application, storing output in files
   -debugger|--debugger <arg0>
                         Sequence of debuggers to search for when "--debug"
                         is used
   -default-hostfile|--default-hostfile <arg0>
                         Provide a default hostfile
   -display-allocation|--display-allocation
                         Display the allocation being used by this job
   -display-devel-allocation|--display-devel-allocation
                         Display a detailed list (mostly intended for
                         developers) of the allocation being used by this
                         job
   -display-devel-map|--display-devel-map
                         Display a detailed process map (mostly intended for
                         developers) just before launch
   -display-map|--display-map
                         Display the process map just before launch
   -do-not-launch|--do-not-launch
                         Perform all necessary operations to prepare to
                         launch the application, but do not actually launch
                         it
   -do-not-resolve|--do-not-resolve
                         Do not attempt to resolve interfaces
   -gmca|--gmca <arg0> <arg1>
                         Pass global MCA parameters that are applicable to
                         all contexts (arg0 is the parameter name; arg1 is
                         the parameter value)
-h|--help                This help message
-H|-host|--host <arg0>   List of hosts to invoke processes on
   --hetero              Indicates that multiple app_contexts are being
                         provided that are a mix of 32/64 bit binaries
   -hostfile|--hostfile <arg0>
                         Provide a hostfile
   -launch-agent|--launch-agent <arg0>
                         Command used to start processes on remote nodes
                         (default: orted)
   -leave-session-attached|--leave-session-attached
                         Enable debugging of OpenRTE
   -loadbalance|--loadbalance
                         Balance total number of procs across all allocated
                         nodes
   -machinefile|--machinefile <arg0>
                         Provide a hostfile
   -mca|--mca <arg0> <arg1>
                         Pass context-specific MCA parameters; they are
                         considered global if --gmca is not used and only
                         one context is specified (arg0 is the parameter
                         name; arg1 is the parameter value)
   -n|--n <arg0>         Number of processes to run
   -nolocal|--nolocal    Do not run any MPI applications on the local node
   -nooversubscribe|--nooversubscribe
                         Nodes are not to be oversubscribed, even if the
                         system supports such operation
   --noprefix            Disable automatic --prefix behavior
   -nperboard|--nperboard <arg0>
                         Launch n processes per board on all allocated
                         nodes
   -npernode|--npernode <arg0>
                         Launch n processes per node on all allocated nodes
   -npersocket|--npersocket <arg0>
                         Launch n processes per socket on all allocated
                         nodes
   -num-boards|--num-boards <arg0>
                         Number of processor boards/node (1-256) [default:
                         1]
   -num-cores|--num-cores <arg0>
                         Number of cores/socket (1-256) [default: 1]
   -num-sockets|--num-sockets <arg0>
                         Number of sockets/board (1-256) [default: 1]
   -ompi-server|--ompi-server <arg0>
                         Specify the URI of the Open MPI server, or the name
                         of the file (specified as file:filename) that
                         contains that info
   -output-filename|--output-filename <arg0>
                         Redirect output from application processes into
                         filename.rank
   -path|--path <arg0>   PATH to be used to look for executables to start
                         processes
   -pernode|--pernode    Launch one process per available node on the
                         specified number of nodes [no -np => use all
                         allocated nodes]
   --prefix <arg0>       Prefix where Open MPI is installed on remote nodes
   --preload-files <arg0>
                         Preload the comma separated list of files to the
                         remote machines current working directory before
                         starting the remote process.
   --preload-files-dest-dir <arg0>
                         The destination directory to use in conjunction
                         with --preload-files. By default the absolute and
                         relative paths provided by --preload-files are
                         used.
-q|--quiet               Suppress helpful messages
   -report-bindings|--report-bindings
                         Whether to report process bindings to stderr
   -report-events|--report-events <arg0>
                         Report events to a tool listening at the specified
                         URI
   -report-pid|--report-pid <arg0>
                         Printout pid on stdout [-], stderr [+], or a file
                         [anything else]
   -report-uri|--report-uri <arg0>
                         Printout URI on stdout [-], stderr [+], or a file
                         [anything else]
   -rf|--rankfile <arg0>
                         Provide a rankfile file
-s|--preload-binary      Preload the binary on the remote machine before
                         starting the remote process.
   -server-wait-time|--server-wait-time <arg0>
                         Time in seconds to wait for ompi-server (default:
                         10 sec)
   -show-progress|--show-progress
                         Output a brief periodic report on launch progress
   -slot-list|--slot-list <arg0>
                         List of processor IDs to bind MPI processes to
                         (e.g., used in conjunction with rank files)
   -stdin|--stdin <arg0>
                         Specify procs to receive stdin [rank, all, none]
                         (default: 0, indicating rank 0)
   -stride|--stride <arg0>
                         When binding multiple cores to a rank, the step
                         size to use between cores [default: 1]
   -tag-output|--tag-output
                         Tag all output with [job,rank]
   -timestamp-output|--timestamp-output
                         Timestamp all application process output
   -tmpdir|--tmpdir <arg0>
                         Set the root for the session directory tree for
                         orterun ONLY
   -tv|--tv              Deprecated backwards compatibility flag; synonym
                         for "--debug"
   -use-regexp|--use-regexp
                         Use regular expressions for launch
-v|--verbose             Be verbose
-V|--version             Print version and exit
   -wait-for-server|--wait-for-server
                         If ompi-server is not already running, wait until
                         it is detected (default: false)
   -wd|--wd <arg0>       Synonym for --wdir
   -wdir|--wdir <arg0>   Set the working directory of the started processes
-x <arg0>                Export an environment variable, optionally
                         specifying a value (e.g., "-x foo" exports the
                         environment variable foo and takes its value from
                         the current environment; "-x foo=bar" exports the
                         environment variable name foo and sets its value to
                         "bar" in the started processes)
   -xml|--xml            Provide all output in XML format
   -xml-file|--xml-file <arg0>
                         Provide all output in XML format to the specified
                         file
   -xterm|--xterm <arg0>
                         Create a new xterm window and display output from
                         the specified ranks there

Report bugs to http://www.open-mpi.org/community/help/
[abj15@login2 6.3.2019]$ mpiexec -np /home/abj15/Research.Project/Software/GenHap/GenHap-master/./compile.sh -o Freq.100_10000SNPs.60x.PacBio
--------------------------------------------------------------------------
mpiexec was unable to launch the specified application as it could not find an executable:

Executable: -o
Node: login2.local

while attempting to start process rank 0.
--------------------------------------------------------------------------
[abj15@login2 6.3.2019]$ mpiexec -np /home/abj15/Research.Project/Software/GenHap/GenHap-master/./compile.sh -i /home/abj15/Research.Project/Software/GenHap/GenHap-master/Models/Freq_100//10000SNPs_60x/PacBio//Model_0.wif -o Freq.100_10000SNPs.60x.PacBio
--------------------------------------------------------------------------
mpiexec was unable to launch the specified application as it could not find an executable:

Executable: -i
Node: login2.local

while attempting to start process rank 0.
--------------------------------------------------------------------------
[abj15@login2 6.3.2019]$ mpiexec -np 1 /home/abj15/Research.Project/Software/GenHap/GenHap-master/./compile.sh -i /home/abj15/Research.Project/Software/GenHap/GenHap-master/Models/Freq_100//10000SNPs_60x/PacBio//Model_0.wif -o Freq.100_10000SNPs.60x.PacBio
--------------------------------------------------------------------------
mpiexec noticed that the job aborted, but has no info as to the process
that caused that situation.
--------------------------------------------------------------------------
[abj15@login2 6.3.2019]$

#########################################################################
# Created by: Allison Bertie Johnson 
# Working Directory: /home/abj15/Research.Project/Run/GenHap/6.5.2019
# Mannual: https://github.com/andrea-tango/GenHap
# GenHap: 6/5/2019
#########################################################################

[abj15@login2 6.5.2019]$ module avail

-------------------------------------------------------------------- /usr/share/Modules/modulefiles --------------------------------------------------------------------
atlas/3.8.4       gcc/7.2.0         intel/14.0.2      intelmpi/14.0.2   java/1.8.0_162    modules           opt-python        rocks-openmpi_ib
dot               gnuplot/5.2.2     intel/15.7.235    intelmpi/15.7.235 module-git        null              R/3.4.0           use.own
fltk/1.3.4-2      gsl/1.16          intel/18          intelmpi/18       module-info       openmpi/2.1.1     rocks-openmpi

--------------------------------------------------------------------------- /etc/modulefiles ---------------------------------------------------------------------------
mpich-x86_64       openmpi-1.8-x86_64 openmpi-x86_64

[abj15@login2 6.5.2019]$ module load gcc/7.2.0
[abj15@login2 6.5.2019]$ module load openmpi/2.1.1
[abj15@login2 6.5.2019]$ module list

Currently Loaded Modulefiles:
  1) rocks-openmpi   2) gcc/7.2.0       3) openmpi/2.1.1

[abj15@login2 6.5.2019]$ /home/abj15/Research.Project/Software/GenHap/GenHap-master/./compile.sh
g++: error: src/genHap.cpp: No such file or directory
g++: error: src/reader.cpp: No such file or directory
g++: error: src/gene.cpp: No such file or directory
g++: error: src/geneticOperation.cpp: No such file or directory
g++: error: src/chromosome.cpp: No such file or directory
g++: error: src/geneticAlgorithm.cpp: No such file or directory

[abj15@login2 6.5.2019]$ /home/abj15/Research.Project/Software/GenHap/GenHap-master/GenHap2
************************************************************************************************************************
GenHap: Genetic Algorithm for Haplotype Assembly, parameters
 * -i: path to the input wif file (mandatory)
 * -o: output folder (optional, default value: output)
 * -n: file name containing the estimated haplotypes (optional, default value haplotypes)
 * -g: number of reads composing each sub-matrix (optional, default value 0)
 * -v: verbose modality (optional, default value 0)
 * -s: save partitions and fragment matrix (optional, default value 0)
 * -x: mask ambiguous positions in the output haplotypes with a X (optional, default value 1)
 * -p: path to the file of GA settings
************************************************************************************************************************

[abj15@login2 6.5.2019]$ mpiexec -np 4 /home/abj15/Research.Project/Software/GenHap/GenHap-master/./GenHap2 -i /home/abj15/Research.Project/Software/GenHap/GenHap-master/Models/Freq_100/10000SNPs_60x/PacBio/Model_0.wif -o Output.Freq_100.10KSNPs_60x.PacBio
************************************************************************************************************************
GenHap: Genetic Algorithm for Haplotype Assembly

 * Using /home/abj15/Research.Project/Software/GenHap/GenHap-master/Models/Freq_100/10000SNPs_60x/PacBio/Model_0.wif as input wif file
 * Using Output.Freq_100.10KSNPs_60x.PacBio as output folder
 * Using haplotypes as file name containing the estimated haplotypes
 * Verbose modality disabled
 * Not saving partitions and fragment matrix
 * Not enabling traditional all-heterozygous assumption
 * Masking ambiguous positions

 * Number of reads: 2999
 * Number of SNPs:  9956

 * Detected 1 haplotype block

 * Start 104 optimizations
 * Using 4 cores
************************************************************************************************************************
Elapsed time 14.57 seconds
************************************************************************************************************************
[abj15@login2 6.5.2019]$ ls
Output.Freq_100.10KSNPs_60x.PacBio
[abj15@login2 6.5.2019]$ cd Output.Freq_100.10KSNPs_60x.PacBio/
[abj15@login2 Output.Freq_100.10KSNPs_60x.PacBio]$ ls
block0  haplotypes0  informations
[abj15@login2 Output.Freq_100.10KSNPs_60x.PacBio]$ more informations
Number of reads: 2999
Number of columns: 9956
Maximum coverage: 48
Minimum coverage: 1
Average coverage: 29
Number of haplotype blocks: 1
Block (0) gamma equal to 29 for each sub-matrix
[abj15@login2 Output.Freq_100.10KSNPs_60x.PacBio]$ more haplotypes0
110100000100111100101110000000110101111010000001010110000111000010011111110110010000101010111111000101011000100111111010011010101101101100101100111010010001100001111010
010011100111010011110101000010001001010110111010110001001110010100001011110101011101010101101001011101110110100100001100011100100001101001111101110111100100011111001011
001010110111101011101110001110101000010011101010111111100100110101100111010110110011100110001011100111110111010110110100111100001001001100110010111001101011010000101111
.
.
.
001000011010011110101100010010110110000100111110100110100101100100100111010000010101011011001111011001011110000001010000001101101001101011110111110111000101110000111101
101100001000101101100011010101111101100101001000110110001100010111100111001110011111000010101110010011001001000110010111010110111010100101110110000111111010000100100101
010100010100110100011001000000110000010000100000010010110011001101000011000000000010000111000011000100011110010001101101111111000010000000011000001111011001001110010111
110110110010000110100011110011101101101011110100111001100011001010010010010000100001000101000100100101110101010110001001101000101111111011110100000010100110111111001011

[abj15@login2 Output.Freq_100.10KSNPs_60x.PacBio]$
[abj15@login2 Output.Freq_100.10KSNPs_60x.PacBio]$ ls
block0  haplotypes0  informations
[abj15@login2 Output.Freq_100.10KSNPs_60x.PacBio]$ cd block0/
[abj15@login2 block0]$ ls
optimization0    optimization15  optimization25  optimization35  optimization45  optimization55  optimization65  optimization75  optimization85  optimization95
optimization1    optimization16  optimization26  optimization36  optimization46  optimization56  optimization66  optimization76  optimization86  optimization96
optimization10   optimization17  optimization27  optimization37  optimization47  optimization57  optimization67  optimization77  optimization87  optimization97
optimization100  optimization18  optimization28  optimization38  optimization48  optimization58  optimization68  optimization78  optimization88  optimization98
optimization101  optimization19  optimization29  optimization39  optimization49  optimization59  optimization69  optimization79  optimization89  optimization99
optimization102  optimization2   optimization3   optimization4   optimization5   optimization6   optimization7   optimization8   optimization9
optimization103  optimization20  optimization30  optimization40  optimization50  optimization60  optimization70  optimization80  optimization90
optimization11   optimization21  optimization31  optimization41  optimization51  optimization61  optimization71  optimization81  optimization91
optimization12   optimization22  optimization32  optimization42  optimization52  optimization62  optimization72  optimization82  optimization92
optimization13   optimization23  optimization33  optimization43  optimization53  optimization63  optimization73  optimization83  optimization93
optimization14   optimization24  optimization34  optimization44  optimization54  optimization64  optimization74  optimization84  optimization94
[abj15@login2 block0]$ more optimization0

*** optimization0: directory ***

[abj15@login2 block0]$ cd optimization0
[abj15@login2 optimization0]$ ls
fitness  haplotypes

[abj15@login2 optimization0]$ more fitness
605.00
488.00
401.00
110.00
103.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
12.00
[abj15@login2 optimization0]$ more haplotypes
110100000100111100101110000000110101111010000001010110000111000010011111110110010000101010111111000101011000100111111010011010101101101100101100111010010001100001111010
01001110011101001-------------------------------------------------------------------------------------------------------------------------------------------------------
.
.
.
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------------

